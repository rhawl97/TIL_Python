{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 머신러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn을 이용한 machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm, metrics\n",
    "# XOR 연산\n",
    "xor_input = [\n",
    "    [0, 0, 0],\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1  2\n",
      "0  0  0  0\n",
      "1  0  1  1\n",
      "2  1  0  1\n",
      "3  1  1  0 \n",
      "\n",
      "   0  1\n",
      "0  0  0\n",
      "1  0  1\n",
      "2  1  0\n",
      "3  1  1\n",
      "정답률 = 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hjkim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "xor_df = pd.DataFrame(xor_input)\n",
    "print(xor_df, \"\\n\")\n",
    "\n",
    "# 입력을 학습 전용 데이터와 테스트 전용 데이터로 분류하기 \n",
    "xor_data  = xor_df[[0,1]] # 데이터 (COLUMN 선택)\n",
    "xor_label = xor_df[2]   # 레이블\n",
    "\n",
    "print(xor_data)\n",
    "    \n",
    "# 데이터 학습과 예측하기 (SVM)\n",
    "clf = svm.SVC()  #학습할 객체 준비\n",
    "clf.fit(xor_data, xor_label) #학습해라\n",
    "pre = clf.predict(xor_data) #예측해라 #데이터가 적어서 학습 데이터 자체를 넣어서 예측해봄\n",
    "\n",
    "# 정답률 구하기 \n",
    "ac_score = metrics.accuracy_score(xor_label, pre)\n",
    "print(\"정답률 =\", ac_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label  prediction\n",
      "0        1           1\n",
      "141      3           3\n",
      "54       1           1\n",
      "129      2           2\n",
      "0.9555555555555556\n",
      "[[15  1  0]\n",
      " [ 1 18  0]\n",
      " [ 0  0 10]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hjkim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "c:\\users\\hjkim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm, metrics\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn import neighbors, metrics, model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "csv = pd.read_csv('wine.csv')\n",
    "csv_data = csv.iloc[ : ,1:14]\n",
    "csv_data = scaler.fit_transform(csv_data)\n",
    "csv_label = csv[\"class\"]\n",
    "train_data, test_data, train_label, test_label = train_test_split(csv_data, csv_label)\n",
    "clf = svm.SVC()\n",
    "#clf = GaussianNB()\n",
    "#clf = MultinomialNB()\n",
    "#clf = neighbors.KNeighborsClassifier()\n",
    "clf.fit(train_data, train_label)\n",
    "pred = clf.predict(test_data)\n",
    "result = pd.DataFrame({\"label\": test_label, \"prediction\": pred})\n",
    "print(result[1:5])\n",
    "acc_score = metrics.accuracy_score(test_label, pred)\n",
    "print(acc_score)\n",
    "print(confusion_matrix(test_label, pred))\n",
    "#scores = model_selection.cross_val_score(clf, csv_data, csv_label, cv=5)\n",
    "# 정답률 구하기 --- (※5)\n",
    "#print(scores)\n",
    "#print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SepalLength  SepalWidth  PetalLength  PetalWidth            Name\n",
      "0            5.1         3.5          1.4         0.2     Iris-setosa\n",
      "1            4.9         3.0          1.4         0.2     Iris-setosa\n",
      "2            4.7         3.2          1.3         0.2     Iris-setosa\n",
      "3            4.6         3.1          1.5         0.2     Iris-setosa\n",
      "4            5.0         3.6          1.4         0.2     Iris-setosa\n",
      "5            5.4         3.9          1.7         0.4     Iris-setosa\n",
      "6            4.6         3.4          1.4         0.3     Iris-setosa\n",
      "7            5.0         3.4          1.5         0.2     Iris-setosa\n",
      "8            4.4         2.9          1.4         0.2     Iris-setosa\n",
      "9            4.9         3.1          1.5         0.1     Iris-setosa\n",
      "10           5.4         3.7          1.5         0.2     Iris-setosa\n",
      "11           4.8         3.4          1.6         0.2     Iris-setosa\n",
      "12           4.8         3.0          1.4         0.1     Iris-setosa\n",
      "13           4.3         3.0          1.1         0.1     Iris-setosa\n",
      "14           5.8         4.0          1.2         0.2     Iris-setosa\n",
      "15           5.7         4.4          1.5         0.4     Iris-setosa\n",
      "16           5.4         3.9          1.3         0.4     Iris-setosa\n",
      "17           5.1         3.5          1.4         0.3     Iris-setosa\n",
      "18           5.7         3.8          1.7         0.3     Iris-setosa\n",
      "19           5.1         3.8          1.5         0.3     Iris-setosa\n",
      "20           5.4         3.4          1.7         0.2     Iris-setosa\n",
      "21           5.1         3.7          1.5         0.4     Iris-setosa\n",
      "22           4.6         3.6          1.0         0.2     Iris-setosa\n",
      "23           5.1         3.3          1.7         0.5     Iris-setosa\n",
      "24           4.8         3.4          1.9         0.2     Iris-setosa\n",
      "25           5.0         3.0          1.6         0.2     Iris-setosa\n",
      "26           5.0         3.4          1.6         0.4     Iris-setosa\n",
      "27           5.2         3.5          1.5         0.2     Iris-setosa\n",
      "28           5.2         3.4          1.4         0.2     Iris-setosa\n",
      "29           4.7         3.2          1.6         0.2     Iris-setosa\n",
      "..           ...         ...          ...         ...             ...\n",
      "120          6.9         3.2          5.7         2.3  Iris-virginica\n",
      "121          5.6         2.8          4.9         2.0  Iris-virginica\n",
      "122          7.7         2.8          6.7         2.0  Iris-virginica\n",
      "123          6.3         2.7          4.9         1.8  Iris-virginica\n",
      "124          6.7         3.3          5.7         2.1  Iris-virginica\n",
      "125          7.2         3.2          6.0         1.8  Iris-virginica\n",
      "126          6.2         2.8          4.8         1.8  Iris-virginica\n",
      "127          6.1         3.0          4.9         1.8  Iris-virginica\n",
      "128          6.4         2.8          5.6         2.1  Iris-virginica\n",
      "129          7.2         3.0          5.8         1.6  Iris-virginica\n",
      "130          7.4         2.8          6.1         1.9  Iris-virginica\n",
      "131          7.9         3.8          6.4         2.0  Iris-virginica\n",
      "132          6.4         2.8          5.6         2.2  Iris-virginica\n",
      "133          6.3         2.8          5.1         1.5  Iris-virginica\n",
      "134          6.1         2.6          5.6         1.4  Iris-virginica\n",
      "135          7.7         3.0          6.1         2.3  Iris-virginica\n",
      "136          6.3         3.4          5.6         2.4  Iris-virginica\n",
      "137          6.4         3.1          5.5         1.8  Iris-virginica\n",
      "138          6.0         3.0          4.8         1.8  Iris-virginica\n",
      "139          6.9         3.1          5.4         2.1  Iris-virginica\n",
      "140          6.7         3.1          5.6         2.4  Iris-virginica\n",
      "141          6.9         3.1          5.1         2.3  Iris-virginica\n",
      "142          5.8         2.7          5.1         1.9  Iris-virginica\n",
      "143          6.8         3.2          5.9         2.3  Iris-virginica\n",
      "144          6.7         3.3          5.7         2.5  Iris-virginica\n",
      "145          6.7         3.0          5.2         2.3  Iris-virginica\n",
      "146          6.3         2.5          5.0         1.9  Iris-virginica\n",
      "147          6.5         3.0          5.2         2.0  Iris-virginica\n",
      "148          6.2         3.4          5.4         2.3  Iris-virginica\n",
      "149          5.9         3.0          5.1         1.8  Iris-virginica\n",
      "\n",
      "[150 rows x 5 columns]\n",
      "     SepalLength  SepalWidth  PetalLength  PetalWidth\n",
      "0            5.1         3.5          1.4         0.2\n",
      "1            4.9         3.0          1.4         0.2\n",
      "2            4.7         3.2          1.3         0.2\n",
      "3            4.6         3.1          1.5         0.2\n",
      "4            5.0         3.6          1.4         0.2\n",
      "5            5.4         3.9          1.7         0.4\n",
      "6            4.6         3.4          1.4         0.3\n",
      "7            5.0         3.4          1.5         0.2\n",
      "8            4.4         2.9          1.4         0.2\n",
      "9            4.9         3.1          1.5         0.1\n",
      "10           5.4         3.7          1.5         0.2\n",
      "11           4.8         3.4          1.6         0.2\n",
      "12           4.8         3.0          1.4         0.1\n",
      "13           4.3         3.0          1.1         0.1\n",
      "14           5.8         4.0          1.2         0.2\n",
      "15           5.7         4.4          1.5         0.4\n",
      "16           5.4         3.9          1.3         0.4\n",
      "17           5.1         3.5          1.4         0.3\n",
      "18           5.7         3.8          1.7         0.3\n",
      "19           5.1         3.8          1.5         0.3\n",
      "20           5.4         3.4          1.7         0.2\n",
      "21           5.1         3.7          1.5         0.4\n",
      "22           4.6         3.6          1.0         0.2\n",
      "23           5.1         3.3          1.7         0.5\n",
      "24           4.8         3.4          1.9         0.2\n",
      "25           5.0         3.0          1.6         0.2\n",
      "26           5.0         3.4          1.6         0.4\n",
      "27           5.2         3.5          1.5         0.2\n",
      "28           5.2         3.4          1.4         0.2\n",
      "29           4.7         3.2          1.6         0.2\n",
      "..           ...         ...          ...         ...\n",
      "120          6.9         3.2          5.7         2.3\n",
      "121          5.6         2.8          4.9         2.0\n",
      "122          7.7         2.8          6.7         2.0\n",
      "123          6.3         2.7          4.9         1.8\n",
      "124          6.7         3.3          5.7         2.1\n",
      "125          7.2         3.2          6.0         1.8\n",
      "126          6.2         2.8          4.8         1.8\n",
      "127          6.1         3.0          4.9         1.8\n",
      "128          6.4         2.8          5.6         2.1\n",
      "129          7.2         3.0          5.8         1.6\n",
      "130          7.4         2.8          6.1         1.9\n",
      "131          7.9         3.8          6.4         2.0\n",
      "132          6.4         2.8          5.6         2.2\n",
      "133          6.3         2.8          5.1         1.5\n",
      "134          6.1         2.6          5.6         1.4\n",
      "135          7.7         3.0          6.1         2.3\n",
      "136          6.3         3.4          5.6         2.4\n",
      "137          6.4         3.1          5.5         1.8\n",
      "138          6.0         3.0          4.8         1.8\n",
      "139          6.9         3.1          5.4         2.1\n",
      "140          6.7         3.1          5.6         2.4\n",
      "141          6.9         3.1          5.1         2.3\n",
      "142          5.8         2.7          5.1         1.9\n",
      "143          6.8         3.2          5.9         2.3\n",
      "144          6.7         3.3          5.7         2.5\n",
      "145          6.7         3.0          5.2         2.3\n",
      "146          6.3         2.5          5.0         1.9\n",
      "147          6.5         3.0          5.2         2.0\n",
      "148          6.2         3.4          5.4         2.3\n",
      "149          5.9         3.0          5.1         1.8\n",
      "\n",
      "[150 rows x 4 columns]\n",
      "정답률 = 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hjkim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 붓꽃의 CSV 데이터 읽어 들이기 --- (※1)\n",
    "csv = pd.read_csv('iris.csv')\n",
    "print(csv)\n",
    "# 필요한 열 추출하기 --- (※2)\n",
    "csv_data = csv[[\"SepalLength\",\"SepalWidth\",\"PetalLength\",\"PetalWidth\"]]\n",
    "print(csv_data)\n",
    "csv_label = csv[\"Name\"]\n",
    "# 학습 전용 데이터와 테스트 전용 데이터로 나누기 --- (※3)\n",
    "train_data, test_data, train_label, test_label = train_test_split(csv_data, csv_label)\n",
    "# 데이터 학습시키고 예측하기 --- (※4)\n",
    "clf = svm.SVC()\n",
    "clf.fit(train_data, train_label)\n",
    "pre = clf.predict(test_data)\n",
    "# 정답률 구하기 --- (※5)\n",
    "ac_score = metrics.accuracy_score(test_label, pre)\n",
    "print(\"정답률 =\", ac_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SepalLength  SepalWidth\n",
      "0          5.1         3.5\n",
      "1          4.9         3.0\n",
      "2          4.7         3.2\n",
      "3          4.6         3.1\n",
      "4          5.0         3.6\n",
      "5          5.4         3.9\n",
      "6          4.6         3.4\n",
      "7          5.0         3.4\n",
      "8          4.4         2.9\n",
      "9          4.9         3.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv = pd.read_csv('iris.csv')\n",
    "print(csv.iloc[0:10,0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PetalWidth\n",
      "0         0.2\n",
      "1         0.2\n",
      "2         0.2\n",
      "3         0.2\n",
      "4         0.2\n",
      "5         0.4\n",
      "6         0.3\n",
      "7         0.2\n",
      "8         0.2\n"
     ]
    }
   ],
   "source": [
    "# X와 y 데이터 구분하는 작업 필요!\n",
    "# 필요한 열 추출하기 \n",
    "csv_data = csv[[\"SepalLength\",\"SepalWidth\",\"PetalLength\",\"PetalWidth\"]]  #DataFrame\n",
    "# csv_data = csv.iloc[:, 0:4]\n",
    "# csv_data = csv.iloc[:, [0,1,2,3]]\n",
    "print(csv_data.iloc[0:9 ,3:4])\n",
    "csv_label = csv[\"Name\"]  # Series\n",
    "# csv_label = csv.iloc[:,4]\n",
    "#print(csv_label[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 학습 전용 데이터와 테스트 전용 데이터로 나누기 \n",
    "train_data, test_data, train_label, test_label = \\  #그 다음 줄을 이어라아\n",
    "train_test_split(csv_data, csv_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답률 = 0.9473684210526315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hjkim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 학습시키고 예측하기 \n",
    "clf = svm.SVC()  #학습 준비\n",
    "clf.fit(train_data, train_label) #학습\n",
    "pre = clf.predict(test_data)  #예측\n",
    "# 정답률 구하기 \n",
    "ac_score = metrics.accuracy_score(test_label, pre)\n",
    "print(\"정답률 =\", ac_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM을 이용한 iris 데이터 분류 full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               label       prediction\n",
      "118   Iris-virginica   Iris-virginica\n",
      "2        Iris-setosa      Iris-setosa\n",
      "80   Iris-versicolor  Iris-versicolor\n",
      "138   Iris-virginica   Iris-virginica\n",
      "\n",
      "정답률 = 0.9473684210526315\n",
      "[[12  0  0]\n",
      " [ 0 11  2]\n",
      " [ 0  0 13]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hjkim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 붓꽃의 CSV 데이터 읽어 들이기 \n",
    "csv = pd.read_csv('iris.csv')\n",
    "\n",
    "# 필요한 열 추출하기 \n",
    "csv_data = csv[[\"SepalLength\",\"SepalWidth\",\"PetalLength\",\"PetalWidth\"]]\n",
    "csv_label = csv[\"Name\"]\n",
    "\n",
    "# 학습 전용 데이터와 테스트 전용 데이터로 나누기 \n",
    "train_data, test_data, train_label, test_label = train_test_split(csv_data, csv_label)\n",
    "\n",
    "# 데이터 학습시키고 예측하기\n",
    "clf = svm.SVC()\n",
    "clf.fit(train_data, train_label)\n",
    "pre = clf.predict(test_data)\n",
    "\n",
    "result = pd.DataFrame({\"label\": test_label, \"prediction\":pre})\n",
    "\n",
    "print(result[1:5])\n",
    "\n",
    "# 정답률 구하기 \n",
    "ac_score = metrics.accuracy_score(test_label, pre)\n",
    "print(\"\\n정답률 =\", ac_score)\n",
    "print(confusion_matrix(test_label, pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2049 60000\n",
      "5\n",
      "0\n",
      "4\n",
      "2051 60000\n",
      "28 28 784\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "\n",
    "# 레이블 파일과 이미지 파일 열기\n",
    "lbl_f = open(\"./mnist/train-labels-idx1-ubyte\", \"rb\")\n",
    "img_f = open(\"./mnist/train-images-idx3-ubyte\", \"rb\")\n",
    "csv_f = open(\"./mnist/train.csv\", \"w\", encoding=\"utf-8\")\n",
    "    \n",
    "# 헤더 정보 읽기 \n",
    "mag, lbl_count = struct.unpack(\">II\", lbl_f.read(8))\n",
    "print(mag, lbl_count)\n",
    "label = struct.unpack(\"B\", lbl_f.read(1))[0] # 레이블 데이터 1개 read\n",
    "print(label)\n",
    "label = struct.unpack(\"B\", lbl_f.read(1))[0] # 레이블 데이터 1개 read\n",
    "print(label)\n",
    "label = struct.unpack(\"B\", lbl_f.read(1))[0] # 레이블 데이터 1개 read\n",
    "print(label)\n",
    "mag, img_count = struct.unpack(\">II\", img_f.read(8))\n",
    "print(mag, img_count)\n",
    "rows, cols = struct.unpack(\">II\", img_f.read(8))\n",
    "pixels = rows * cols\n",
    "print(rows, cols, pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 이미지 데이터를 읽고 CSV로 저장하기 \n",
    "    res = []\n",
    "    for idx in range(lbl_count):\n",
    "        if idx > 1000: break   # 1000 개만 load\n",
    "        label = struct.unpack(\"B\", lbl_f.read(1))[0] # 레이블 데이터 1개 read\n",
    "        bdata = img_f.read(pixels)   # image data 28*28 read\n",
    "        sdata = list(map(lambda n: str(n), bdata)) # 한 pixel씩 문자열로 변환한 후에 list로 변환\n",
    "        \n",
    "        # csv로 저장\n",
    "        csv_f.write(str(label)+\",\")\n",
    "        csv_f.write(\",\".join(sdata)+\"\\r\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "        if idx < 10:\n",
    "            s = \"P2 28 28 255\\n\"\n",
    "            s += \" \".join(sdata)\n",
    "            iname = \"./mnist/{0}-{1}-{2}.pgm\".format(name,idx,label)\n",
    "            with open(iname, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv 변환 full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "def to_csv(name, maxdata):\n",
    "    # 레이블 파일과 이미지 파일 열기\n",
    "    lbl_f = open(\"./mnist/\"+name+\"-labels-idx1-ubyte\", \"rb\")\n",
    "    img_f = open(\"./mnist/\"+name+\"-images-idx3-ubyte\", \"rb\")\n",
    "    csv_f = open(\"./mnist/\"+name+\".csv\", \"w\", encoding=\"utf-8\")\n",
    "    # 헤더 정보 읽기 \n",
    "    mag, lbl_count = struct.unpack(\">II\", lbl_f.read(8))\n",
    "    mag, img_count = struct.unpack(\">II\", img_f.read(8))\n",
    "    rows, cols = struct.unpack(\">II\", img_f.read(8))\n",
    "    pixels = rows * cols\n",
    "    # 이미지 데이터를 읽고 CSV로 저장하기 \n",
    "    res = []\n",
    "    for idx in range(lbl_count):\n",
    "        if idx > maxdata: break\n",
    "        label = struct.unpack(\"B\", lbl_f.read(1))[0]\n",
    "        bdata = img_f.read(pixels)\n",
    "        sdata = list(map(lambda n: str(n), bdata))\n",
    "        csv_f.write(str(label)+\",\")\n",
    "        csv_f.write(\",\".join(sdata)+\"\\r\\n\")\n",
    "        # 잘 저장됐는지 이미지 파일로 저장해서 테스트하기 \n",
    "        if idx < 10:\n",
    "            s = \"P2 28 28 255\\n\"\n",
    "            s += \" \".join(sdata)\n",
    "            iname = \"./mnist/{0}-{1}-{2}.pgm\".format(name,idx,label)\n",
    "            with open(iname, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(s)\n",
    "    csv_f.close()\n",
    "    lbl_f.close()\n",
    "    img_f.close()\n",
    "# 결과를 파일로 출력하기 \n",
    "to_csv(\"train\", 60000)\n",
    "to_csv(\"t10k\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm, metrics\n",
    "\n",
    "train = pd.read_csv('./mnist/train.csv')\n",
    "test = pd.read_csv('./mnist/t10k.csv')\n",
    "\n",
    "# 필요한 열 추출하기 \n",
    "train_images = train.iloc[:, 1:]\n",
    "train_label = train.iloc[:, 0]\n",
    "test_images = test.iloc[:, 1:]\n",
    "test_label = test.iloc[:, 0]\n",
    "\n",
    "print(train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 ... 5 6 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hjkim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm, metrics\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# 붓꽃의 CSV 데이터 읽어 들이기 \n",
    "#train = pd.read_csv('./mnist/train.csv')\n",
    "#test = pd.read_csv('./mnist/t10k.csv')\n",
    "\n",
    "# 필요한 열 추출하기 \n",
    "#train_images = train.iloc[:, 1:]\n",
    "#train_label = train.iloc[:, 0]\n",
    "#test_images = test.iloc[:, 1:]\n",
    "#test_label = test.iloc[:, 0]\n",
    "\n",
    "# 0과 1사이의 실수로 정규화\n",
    "\n",
    "\n",
    "(train_images, train_label), (test_images, test_label) = mnist.load_data()\n",
    "train_images = train_images.reshape(60000, 784)\n",
    "test_images = test_images.reshape(10000, 784)\n",
    "#train_images = train_images.astype('float32')\n",
    "#test_images = test_images.astype('float32')\n",
    "#train_images = train_images/255\n",
    "#test_images = test_images/255\n",
    "print(train_label)\n",
    "\n",
    "# 데이터 학습시키고 예측하기\n",
    "clf = svm.SVC()\n",
    "clf.fit(train_images, train_label)\n",
    "pre = clf.predict(test_images)\n",
    "\n",
    "result = pd.DataFrame({\"label\": test_label, \"prediction\":pre})\n",
    "\n",
    "print(result[0:10])\n",
    "\n",
    "# 정답률 구하기 \n",
    "ac_score = metrics.accuracy_score(test_label, pre)\n",
    "print(\"\\n정답률 =\", ac_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   col\n",
      "0   -3\n",
      "1   -1\n",
      "2    1\n",
      "3    3\n",
      "4    5\n",
      "5    7\n",
      "6    9\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-52f490d6aa07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# 0과 과 1 사이의 값으로 변환\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"minmax_scale\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminmax_scale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hjkim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mminmax_scale\u001b[1;34m(X, feature_range, axis, copy)\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;31m# If copy is required, it will be done inside the scaler object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     X = check_array(X, copy=False, ensure_2d=False, warn_on_dtype=True,\n\u001b[1;32m--> 462\u001b[1;33m                     dtype=FLOAT_DTYPES, force_all_finite='allow-nan')\n\u001b[0m\u001b[0;32m    463\u001b[0m     \u001b[0moriginal_ndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hjkim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     if (warn_on_dtype and dtypes_orig is not None and\n\u001b[1;32m--> 596\u001b[1;33m             {array.dtype} != set(dtypes_orig)):\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;31m# if there was at the beginning some other types than the final one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# (for instance in a DataFrame that can contain several dtypes) then\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: iteration over a 0-d array"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import scale, minmax_scale\n",
    "\n",
    "x = pd.DataFrame({'col':[-3, -1, 1, 3, 5, 7, 9]})\n",
    "print(x)\n",
    "# 평균 0, 분산을 이용해 정규화\n",
    "# astype(float)는 scale의 입력이 float이므로 warning방지를 위해 변환\n",
    "#x[\"scale\"] = scale(x.col.astype(float))\n",
    "#x[\"scale\"] = scale(x.col)\n",
    "\n",
    "# 0과 과 1 사이의 값으로 변환\n",
    "x[\"minmax_scale\"] = minmax_scale(x.col.astype(float))\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A         B      C\n",
      "0  0.000000  0.000000    big\n",
      "1  0.926219  0.363636  small\n",
      "2  0.935335  0.628645    big\n",
      "3  1.000000  0.961407  small\n",
      "4  0.938495  1.000000  small\n",
      "          A         B      C\n",
      "0  0.000000  0.000000    big\n",
      "1  0.926219  0.363636  small\n",
      "2  0.935335  0.628645    big\n",
      "3  1.000000  0.961407  small\n",
      "4  0.938495  1.000000  small\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import scale, minmax_scale\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "dfTest = pd.DataFrame({'A':[14.00,90.20,90.95,96.27,91.21],\n",
    "                       'B':[103.02,107.26,110.35,114.23,114.68],\n",
    "                       'C':['big','small','big','small','small']})\n",
    "\n",
    "dfTest[['A', 'B']] = scaler.fit_transform(dfTest[['A', 'B']])\n",
    "print(dfTest)\n",
    "dfTest[['A', 'B']] = minmax_scale(dfTest[['A', 'B']])\n",
    "\n",
    "\n",
    "print(dfTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amsterdam' 'paris' 'tokyo']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "r = le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amsterdam' 'paris' 'tokyo']\n",
      "<class 'numpy.ndarray'> \n",
      "\n",
      "[2 2 1 0]\n",
      "<class 'numpy.ndarray'> \n",
      "\n",
      "['tokyo' 'tokyo' 'paris']\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
    "print(le.classes_)\n",
    "print(type(le.classes_), \"\\n\")\n",
    "\n",
    "data = le.transform([\"tokyo\", \"tokyo\", \"paris\", \"amsterdam\"]) \n",
    "print(data)\n",
    "print(type(data), \"\\n\")\n",
    "\n",
    "original = le.inverse_transform([2, 2, 1])\n",
    "print(original)\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't convert 'int' object to str implicitly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-a229b6970b5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#for i in range(ord('a'), ord('z')+1):  # ord('a'): 'a'이 ascii code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'z'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# ord('a'): 'a'이 ascii code\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m                 \u001b[1;31m# chr(i): ascii code i에 해당하는 문자\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Can't convert 'int' object to str implicitly"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "str = []\n",
    "for i in range(ord('a'), ord('z')+1):  # ord('a'): 'a'이 ascii code\n",
    "#for i in range('a', 'z'+1):  # ord('a'): 'a'이 ascii code\n",
    "    str.append(i)                 # chr(i): ascii code i에 해당하는 문자\n",
    "print(str)\n",
    "\n",
    "le.fit(str)\n",
    "data = le.transform(['q', 'a', 'z']) \n",
    "print(data)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B\n",
      "0  0  0\n",
      "1  1  1\n",
      "2  1  0\n",
      "3  2  1\n",
      "4  0  0\n",
      "<class 'pandas.core.frame.DataFrame'> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "df = pd.DataFrame({'A': ['a', 'b', 'b', 'c', 'a'],\n",
    "                   'B': ['x', 'y', 'x', 'y', 'x']})\n",
    "\n",
    "# fit_transform: fit과 tranform을 동시에 처리\n",
    "# df.apply는 dataframe에서 인자로 주어진 함수를 각 column에 적용하는 함수\n",
    "data = df.apply(le.fit_transform)\n",
    "print(data)\n",
    "print(type(data), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   country_australia  country_germany  country_korea  country_russia\n",
      "0                  0                0              0               1\n",
      "1                  0                1              0               0\n",
      "2                  1                0              0               0\n",
      "3                  0                0              1               0\n",
      "4                  0                1              0               0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'country': ['russia', 'germany', 'australia','korea','germany']})\n",
    "a = pd.get_dummies(df,prefix=['country'])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B\n",
      "0  a  x\n",
      "1  b  y\n",
      "2  b  x\n",
      "3  c  y\n",
      "4  a  x\n",
      "   A_a  A_b  A_c  B_x  B_y\n",
      "0    1    0    0    1    0\n",
      "1    0    1    0    0    1\n",
      "2    0    1    0    1    0\n",
      "3    0    0    1    0    1\n",
      "4    1    0    0    1    0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'A': ['a', 'b', 'b', 'c', 'a'],\n",
    "                   'B': ['x', 'y', 'x', 'y', 'x']})\n",
    "print(df)\n",
    "a = pd.get_dummies(df,prefix=['A', 'B'])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A_a  A_b  A_c  B_2  B_3  B_4  B_5  B_7\n",
      "0    1    0    0    0    1    0    0    0\n",
      "1    0    1    0    0    0    1    0    0\n",
      "2    0    1    0    0    0    0    0    1\n",
      "3    0    0    1    1    0    0    0    0\n",
      "4    1    0    0    0    0    0    1    0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'A': ['a', 'b', 'b', 'c', 'a'],\n",
    "                                        'B': [3,4, 7, 2, 5]})\n",
    "a = pd.get_dummies(df,prefix=['A'])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   country\n",
      "0        3\n",
      "1        1\n",
      "2        0\n",
      "3        2\n",
      "4        1 <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      "\n",
      "  (0, 3)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (3, 2)\t1.0\n",
      "  (4, 1)\t1.0 <class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]] <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hjkim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = pd.DataFrame({'country': ['russia', 'germany', 'australia','korea','germany']})\n",
    "\n",
    "# 곧바로 onehot encoding이 안됨. data frame 전체를 라벨인코딩후(숫자로 변환 후)에 원핫인코딩을 해야함)\n",
    "le = LabelEncoder()\n",
    "X2 = X.apply(le.fit_transform)\n",
    "print(X2, type(X2))\n",
    "print(\"\\n\")\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "X2 = encoder.fit_transform(X2)  #  결과는 sparse matrix로 변환됨\n",
    "print(X2, type(X2))\n",
    "print(\"\\n\")\n",
    "\n",
    "X3 = X2.toarray() # numpy array로 변환. 추후에 dataframe으로 변환\n",
    "print(X3, type(X3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B\n",
      "0  0  0\n",
      "1  1  1\n",
      "2  1  0\n",
      "3  2  1\n",
      "4  0  0 <class 'pandas.core.frame.DataFrame'>\n",
      "  (0, 3)\t1.0\n",
      "  (0, 0)\t1.0\n",
      "  (1, 4)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 3)\t1.0\n",
      "  (2, 1)\t1.0\n",
      "  (3, 4)\t1.0\n",
      "  (3, 2)\t1.0\n",
      "  (4, 3)\t1.0\n",
      "  (4, 0)\t1.0 <class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[ 1.  0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.  1.]\n",
      " [ 0.  1.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  1.]\n",
      " [ 1.  0.  0.  1.  0.]] <class 'numpy.ndarray'>\n",
      "     0    1    2    3    4\n",
      "0  1.0  0.0  0.0  1.0  0.0\n",
      "1  0.0  1.0  0.0  0.0  1.0\n",
      "2  0.0  1.0  0.0  1.0  0.0\n",
      "3  0.0  0.0  1.0  0.0  1.0\n",
      "4  1.0  0.0  0.0  1.0  0.0 <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = pd.DataFrame({'A': ['a', 'b', 'b', 'c', 'a'],\n",
    "                   'B': ['x', 'y', 'x', 'y', 'x']})\n",
    "\n",
    "# 곧바로 onehot encoding이 안됨. datafframe 전체를 라벨인코딩후(숫자로 변환 후)에 원핫인코딩을 해야함)\n",
    "le = LabelEncoder()\n",
    "X2 = X.apply(le.fit_transform)\n",
    "print(X2, type(X2))\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "X2 = encoder.fit_transform(X2)  \n",
    "#  결과는 sparse matrix로 변환됨. 더이상 dataframe이 아니므로 column명이 소실됨\n",
    "print(X2, type(X2))\n",
    "\n",
    "X3 = X2.toarray() # numpy array로 변환. 추후에 dataframe으로 변환\n",
    "print(X3, type(X3))\n",
    "\n",
    "X3 = pd.DataFrame(X3) # 최종적으로 dataframe으로 다시 변환\n",
    "print(X3, type(X3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B\n",
      "0  0  3\n",
      "1  1  4\n",
      "2  1  5\n",
      "3  2  1\n",
      "4  0  7\n",
      "  (0, 0)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 1)\t1.0\n",
      "  (3, 2)\t1.0\n",
      "  (4, 0)\t1.0\n",
      "  (0, 3)\t3.0\n",
      "  (1, 3)\t4.0\n",
      "  (2, 3)\t5.0\n",
      "  (3, 3)\t1.0\n",
      "  (4, 3)\t7.0 <class 'scipy.sparse.coo.coo_matrix'>\n",
      "[[ 1.  0.  0.  3.]\n",
      " [ 0.  1.  0.  4.]\n",
      " [ 0.  1.  0.  5.]\n",
      " [ 0.  0.  1.  1.]\n",
      " [ 1.  0.  0.  7.]] <class 'numpy.ndarray'>\n",
      "     0    1    2    3\n",
      "0  1.0  0.0  0.0  3.0\n",
      "1  0.0  1.0  0.0  4.0\n",
      "2  0.0  1.0  0.0  5.0\n",
      "3  0.0  0.0  1.0  1.0\n",
      "4  1.0  0.0  0.0  7.0 <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = pd.DataFrame({'A': ['a', 'b', 'b', 'c', 'a'],\n",
    "                  'B': [3, 4, 5, 1, 7]})\n",
    "\n",
    "le = LabelEncoder()\n",
    "X.A = le.fit_transform(X.A) \n",
    "# X.A는 series이므로 apply함수를 적용하지 않고 fit_transform이 인자로 넣어야 함\n",
    "print(X)\n",
    "\n",
    "# onehot encoding이 필요한 column만 지정\n",
    "encoder = OneHotEncoder(categorical_features=[True, False])\n",
    "\n",
    "X2 = encoder.fit_transform(X)  #  결과는 sparse matrix로 변환됨\n",
    "print(X2, type(X2))\n",
    "\n",
    "X3 = X2.toarray() # numpy array로 변환. \n",
    "print(X3, type(X3))\n",
    "\n",
    "X3 = pd.DataFrame(X3) # 최종적으로 dataframe으로 다시 변환\n",
    "print(X3, type(X3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0    1   2   3    4     5      6   7  8\n",
      "0   6  148  72  35    0  33.6  0.627  50  1\n",
      "1   1   85  66  29    0  26.6  0.351  31  0\n",
      "2   8  183  64   0    0  23.3  0.672  32  1\n",
      "3   1   89  66  23   94  28.1  0.167  21  0\n",
      "4   0  137  40  35  168  43.1  2.288  33  1\n",
      "5   5  116  74   0    0  25.6  0.201  30  0\n",
      "6   3   78  50  32   88  31.0  0.248  26  1\n",
      "7  10  115   0   0    0  35.3  0.134  29  0\n",
      "8   2  197  70  45  543  30.5  0.158  53  1\n",
      "9   8  125  96   0    0   0.0  0.232  54  1 \n",
      "\n",
      "       0      1      2      3      4      5      6      7      8\n",
      "0  False  False  False  False   True  False  False  False  False\n",
      "1  False  False  False  False   True  False  False  False   True\n",
      "2  False  False  False   True   True  False  False  False  False\n",
      "3  False  False  False  False  False  False  False  False   True\n",
      "4   True  False  False  False  False  False  False  False  False\n",
      "5  False  False  False   True   True  False  False  False   True\n",
      "6  False  False  False  False  False  False  False  False  False\n",
      "7  False  False   True   True   True  False  False  False   True\n",
      "8  False  False  False  False  False  False  False  False  False\n",
      "9  False  False  False   True   True   True  False  False  False\n",
      "0    1\n",
      "1    0\n",
      "2    1\n",
      "3    4\n",
      "4    6\n",
      "5    1\n",
      "6    0\n",
      "7    0\n",
      "8    4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('pima-indians-diabetes.csv', header=None)\n",
    "print(dataset[0:10], \"\\n\")\n",
    "print((dataset[0:10] == 0))\n",
    "print((dataset[0:10] == 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0      1     2     3      4     5      6   7  8\n",
      "0    6  148.0  72.0  35.0    NaN  33.6  0.627  50  1\n",
      "1    1   85.0  66.0  29.0    NaN  26.6  0.351  31  0\n",
      "2    8  183.0  64.0   NaN    NaN  23.3  0.672  32  1\n",
      "3    1   89.0  66.0  23.0   94.0  28.1  0.167  21  0\n",
      "4    0  137.0  40.0  35.0  168.0  43.1  2.288  33  1\n",
      "5    5  116.0  74.0   NaN    NaN  25.6  0.201  30  0\n",
      "6    3   78.0  50.0  32.0   88.0  31.0  0.248  26  1\n",
      "7   10  115.0   NaN   NaN    NaN  35.3  0.134  29  0\n",
      "8    2  197.0  70.0  45.0  543.0  30.5  0.158  53  1\n",
      "9    8  125.0  96.0   NaN    NaN   NaN  0.232  54  1\n",
      "10   4  110.0  92.0   NaN    NaN  37.6  0.191  30  0\n",
      "11  10  168.0  74.0   NaN    NaN  38.0  0.537  34  1\n",
      "12  10  139.0  80.0   NaN    NaN  27.1  1.441  57  0\n",
      "13   1  189.0  60.0  23.0  846.0  30.1  0.398  59  1\n",
      "14   5  166.0  72.0  19.0  175.0  25.8  0.587  51  1\n",
      "15   7  100.0   NaN   NaN    NaN  30.0  0.484  32  1\n",
      "16   0  118.0  84.0  47.0  230.0  45.8  0.551  31  1\n",
      "17   7  107.0  74.0   NaN    NaN  29.6  0.254  31  1\n",
      "18   1  103.0  30.0  38.0   83.0  43.3  0.183  33  0\n",
      "19   1  115.0  70.0  30.0   96.0  34.6  0.529  32  1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
    "\n",
    "# 1부터 5컬럼까지에서 0을  NAN으로 바꾸고 그 결과를 dataset에 저장\n",
    "# mark zero values as missing or NaN\n",
    "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, np.NaN)\n",
    "\n",
    "# print the first 20 rows of data\n",
    "print(dataset.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n",
      "(392, 9)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset = pd.read_csv('pima-indians-diabetes.csv', header=None)\n",
    "\n",
    "# mark zero values as missing or NaN\n",
    "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, np.NaN)\n",
    "print(dataset.shape)\n",
    "\n",
    "# drop rows with missing values\n",
    "dataset.dropna(inplace=True)\n",
    "\n",
    "# summarize the number of rows and columns in the dataset\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      3.845052\n",
      "1    121.686763\n",
      "2     72.405184\n",
      "3     29.153420\n",
      "4    155.548223\n",
      "5     32.457464\n",
      "6      0.471876\n",
      "7     33.240885\n",
      "8      0.348958\n",
      "dtype: float64\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "5    0\n",
      "6    0\n",
      "7    0\n",
      "8    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset = pd.read_csv('pima-indians-diabetes.csv', header=None)\n",
    "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, numpy.NaN)\n",
    "\n",
    "print(dataset.mean())\n",
    "\n",
    "# fill missing values with mean column values\n",
    "dataset.fillna(dataset.mean(), inplace=True)\n",
    "\n",
    "# count the number of NaN values in each column\n",
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as req\n",
    "local= \"mushroom.csv\"\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\"\n",
    "req.urlretrieve(url, local)\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0  1_b  1_c  1_f  1_k  1_s  1_x  2_f  2_g  2_s  ...   21_s  21_v  21_y  \\\n",
      "0     p    0    0    0    0    0    1    0    0    1  ...      1     0     0   \n",
      "1     e    0    0    0    0    0    1    0    0    1  ...      0     0     0   \n",
      "2     e    1    0    0    0    0    0    0    0    1  ...      0     0     0   \n",
      "3     p    0    0    0    0    0    1    0    0    0  ...      1     0     0   \n",
      "4     e    0    0    0    0    0    1    0    0    1  ...      0     0     0   \n",
      "5     e    0    0    0    0    0    1    0    0    0  ...      0     0     0   \n",
      "6     e    1    0    0    0    0    0    0    0    1  ...      0     0     0   \n",
      "7     e    1    0    0    0    0    0    0    0    0  ...      1     0     0   \n",
      "8     p    0    0    0    0    0    1    0    0    0  ...      0     1     0   \n",
      "9     e    1    0    0    0    0    0    0    0    1  ...      1     0     0   \n",
      "10    e    0    0    0    0    0    1    0    0    0  ...      0     0     0   \n",
      "11    e    0    0    0    0    0    1    0    0    0  ...      1     0     0   \n",
      "12    e    1    0    0    0    0    0    0    0    1  ...      1     0     0   \n",
      "13    p    0    0    0    0    0    1    0    0    0  ...      0     1     0   \n",
      "14    e    0    0    0    0    0    1    1    0    0  ...      0     0     0   \n",
      "15    e    0    0    0    0    1    0    1    0    0  ...      0     0     1   \n",
      "16    e    0    0    1    0    0    0    1    0    0  ...      0     0     0   \n",
      "17    p    0    0    0    0    0    1    0    0    1  ...      1     0     0   \n",
      "18    p    0    0    0    0    0    1    0    0    0  ...      1     0     0   \n",
      "19    p    0    0    0    0    0    1    0    0    1  ...      1     0     0   \n",
      "20    e    1    0    0    0    0    0    0    0    1  ...      1     0     0   \n",
      "21    p    0    0    0    0    0    1    0    0    0  ...      0     1     0   \n",
      "22    e    1    0    0    0    0    0    0    0    0  ...      1     0     0   \n",
      "23    e    1    0    0    0    0    0    0    0    0  ...      0     0     0   \n",
      "24    e    1    0    0    0    0    0    0    0    1  ...      1     0     0   \n",
      "25    p    0    0    1    0    0    0    0    0    1  ...      0     1     0   \n",
      "26    e    0    0    0    0    0    1    0    0    0  ...      0     0     0   \n",
      "27    e    0    0    0    0    0    1    0    0    0  ...      0     0     0   \n",
      "28    e    0    0    1    0    0    0    1    0    0  ...      0     0     1   \n",
      "29    e    0    0    0    0    0    1    0    0    1  ...      0     1     0   \n",
      "...  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...   ...   ...   \n",
      "8094  e    1    0    0    0    0    0    0    0    1  ...      0     0     0   \n",
      "8095  p    0    0    0    0    0    1    0    0    0  ...      0     0     0   \n",
      "8096  e    0    0    0    1    0    0    1    0    0  ...      0     0     0   \n",
      "8097  p    0    0    0    1    0    0    0    0    0  ...      0     1     0   \n",
      "8098  p    0    0    0    1    0    0    0    0    1  ...      0     1     0   \n",
      "8099  e    0    0    0    1    0    0    1    0    0  ...      1     0     0   \n",
      "8100  e    0    0    1    0    0    0    0    0    1  ...      0     1     0   \n",
      "8101  p    0    0    0    1    0    0    0    0    1  ...      0     1     0   \n",
      "8102  e    0    0    0    0    0    1    0    0    1  ...      0     0     0   \n",
      "8103  e    0    0    0    1    0    0    0    0    1  ...      0     0     0   \n",
      "8104  e    0    0    0    1    0    0    0    0    1  ...      0     1     0   \n",
      "8105  e    0    0    0    1    0    0    0    0    1  ...      0     1     0   \n",
      "8106  e    0    0    0    1    0    0    0    0    1  ...      0     1     0   \n",
      "8107  e    0    0    0    0    0    1    0    0    1  ...      0     0     0   \n",
      "8108  p    0    0    0    1    0    0    0    0    0  ...      0     1     0   \n",
      "8109  e    1    0    0    0    0    0    0    0    1  ...      0     0     0   \n",
      "8110  e    0    0    0    0    0    1    0    0    1  ...      0     1     0   \n",
      "8111  e    0    0    0    1    0    0    0    0    1  ...      0     0     0   \n",
      "8112  e    0    0    0    1    0    0    0    0    1  ...      0     1     0   \n",
      "8113  p    0    0    0    1    0    0    0    0    0  ...      0     1     0   \n",
      "8114  p    0    0    1    0    0    0    0    0    0  ...      0     0     0   \n",
      "8115  e    0    0    0    0    0    1    0    0    1  ...      0     1     0   \n",
      "8116  p    0    0    0    1    0    0    0    0    0  ...      0     1     0   \n",
      "8117  p    0    0    0    1    0    0    0    0    1  ...      0     1     0   \n",
      "8118  p    0    0    0    1    0    0    0    0    0  ...      0     1     0   \n",
      "8119  e    0    0    0    1    0    0    0    0    1  ...      0     0     0   \n",
      "8120  e    0    0    0    0    0    1    0    0    1  ...      0     1     0   \n",
      "8121  e    0    0    1    0    0    0    0    0    1  ...      0     0     0   \n",
      "8122  p    0    0    0    1    0    0    0    0    0  ...      0     1     0   \n",
      "8123  e    0    0    0    0    0    1    0    0    1  ...      0     0     0   \n",
      "\n",
      "      22_d  22_g  22_l  22_m  22_p  22_u  22_w  \n",
      "0        0     0     0     0     0     1     0  \n",
      "1        0     1     0     0     0     0     0  \n",
      "2        0     0     0     1     0     0     0  \n",
      "3        0     0     0     0     0     1     0  \n",
      "4        0     1     0     0     0     0     0  \n",
      "5        0     1     0     0     0     0     0  \n",
      "6        0     0     0     1     0     0     0  \n",
      "7        0     0     0     1     0     0     0  \n",
      "8        0     1     0     0     0     0     0  \n",
      "9        0     0     0     1     0     0     0  \n",
      "10       0     1     0     0     0     0     0  \n",
      "11       0     0     0     1     0     0     0  \n",
      "12       0     1     0     0     0     0     0  \n",
      "13       0     0     0     0     0     1     0  \n",
      "14       0     1     0     0     0     0     0  \n",
      "15       0     0     0     0     0     1     0  \n",
      "16       0     1     0     0     0     0     0  \n",
      "17       0     1     0     0     0     0     0  \n",
      "18       0     0     0     0     0     1     0  \n",
      "19       0     0     0     0     0     1     0  \n",
      "20       0     0     0     1     0     0     0  \n",
      "21       0     1     0     0     0     0     0  \n",
      "22       0     0     0     1     0     0     0  \n",
      "23       0     0     0     1     0     0     0  \n",
      "24       0     0     0     1     0     0     0  \n",
      "25       0     1     0     0     0     0     0  \n",
      "26       0     0     0     1     0     0     0  \n",
      "27       0     0     0     1     0     0     0  \n",
      "28       0     0     0     0     0     1     0  \n",
      "29       1     0     0     0     0     0     0  \n",
      "...    ...   ...   ...   ...   ...   ...   ...  \n",
      "8094     0     1     0     0     0     0     0  \n",
      "8095     1     0     0     0     0     0     0  \n",
      "8096     0     1     0     0     0     0     0  \n",
      "8097     0     0     1     0     0     0     0  \n",
      "8098     1     0     0     0     0     0     0  \n",
      "8099     0     1     0     0     0     0     0  \n",
      "8100     0     0     1     0     0     0     0  \n",
      "8101     0     0     0     0     1     0     0  \n",
      "8102     0     0     1     0     0     0     0  \n",
      "8103     0     0     1     0     0     0     0  \n",
      "8104     0     0     1     0     0     0     0  \n",
      "8105     0     0     1     0     0     0     0  \n",
      "8106     0     0     1     0     0     0     0  \n",
      "8107     0     0     1     0     0     0     0  \n",
      "8108     0     0     1     0     0     0     0  \n",
      "8109     0     1     0     0     0     0     0  \n",
      "8110     0     0     1     0     0     0     0  \n",
      "8111     0     1     0     0     0     0     0  \n",
      "8112     0     0     1     0     0     0     0  \n",
      "8113     1     0     0     0     0     0     0  \n",
      "8114     1     0     0     0     0     0     0  \n",
      "8115     0     0     1     0     0     0     0  \n",
      "8116     0     0     1     0     0     0     0  \n",
      "8117     1     0     0     0     0     0     0  \n",
      "8118     1     0     0     0     0     0     0  \n",
      "8119     0     0     1     0     0     0     0  \n",
      "8120     0     0     1     0     0     0     0  \n",
      "8121     0     0     1     0     0     0     0  \n",
      "8122     0     0     1     0     0     0     0  \n",
      "8123     0     0     1     0     0     0     0  \n",
      "\n",
      "[8124 rows x 118 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터 읽어 들이기\n",
    "mr = pd.read_csv(\"mushroom.csv\", header=None)\n",
    "\n",
    "# label 분리\n",
    "df = pd.DataFrame(mr.iloc[:, 0]) # 1 column만 선택하면 Series가 되므로 다시 dataframe으로 만듦\n",
    "\n",
    "# 두번쨰 컬럼부터 마지막 컬럼까지 one-hot encoding하고 label에 붙임\n",
    "df = df.join(pd.get_dummies(mr.iloc[:, 1:]))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label pre\n",
      "1473     e   e\n",
      "8103     e   e\n",
      "1289     e   e\n",
      "5275     p   p\n",
      "5365     p   p\n",
      "1320     e   e\n",
      "5997     e   e\n",
      "6451     p   p\n",
      "7465     p   p\n",
      "4504     p   p\n",
      "정답률 = 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터 읽어 들이기\n",
    "mr = pd.read_csv(\"mushroom.csv\", header=None)\n",
    "\n",
    "# label 분리\n",
    "df = pd.DataFrame(mr.iloc[:, 0])\n",
    "\n",
    "# 두번쨰 컬럼부터 마지막 컬럼까지 one-hot encoding하고 label에 붙임\n",
    "df = df.join(pd.get_dummies(mr.iloc[:, 1:]))\n",
    "\n",
    "\n",
    "data = df.iloc[:, 1:]\n",
    "label = df.loc[:, 0]\n",
    "\n",
    "# 학습 전용 데이터와 테스트 전용 데이터로 나누기\n",
    "data_train, data_test, label_train, label_test = train_test_split(data, label)\n",
    "\n",
    "# 데이터 학습시키기\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(data_train, label_train)\n",
    "\n",
    "# 데이터 예측하기\n",
    "predict = clf.predict(data_test)\n",
    "\n",
    "# 결과 테스트하기\n",
    "result = pd.DataFrame({\"label\": label_test, \"pre\": predict})\n",
    "print(result[0:10])\n",
    "\n",
    "ac_score = metrics.accuracy_score(label_test, predict)\n",
    "print(\"정답률 =\", ac_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답률 = 0.868421052631579\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import neighbors, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 붓꽃의 CSV 데이터 읽어 들이기 \n",
    "csv = pd.read_csv('iris.csv')\n",
    "\n",
    "# 필요한 열 추출하기 \n",
    "csv_data = csv[[\"SepalLength\",\"SepalWidth\",\"PetalLength\",\"PetalWidth\"]]\n",
    "csv_label = csv[\"Name\"]\n",
    "\n",
    "# 학습 전용 데이터와 테스트 전용 데이터로 나누기 --- (※3)\n",
    "train_data, test_data, train_label, test_label = \\\n",
    "train_test_split(csv_data, csv_label)\n",
    "\n",
    "# 데이터 학습시키고 예측하기 \n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=19)\n",
    "knn.fit(train_data, train_label)\n",
    "pre = knn.predict(test_data)\n",
    "\n",
    "# 정답률 구하기 \n",
    "ac_score = metrics.accuracy_score(test_label, pre)\n",
    "print(\"정답률 =\", ac_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답률 = 0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 붓꽃의 CSV 데이터 읽어 들이기 \n",
    "csv = pd.read_csv('iris.csv')\n",
    "\n",
    "# 필요한 열 추출하기 \n",
    "csv_data = csv[[\"SepalLength\",\"SepalWidth\",\"PetalLength\",\"PetalWidth\"]]\n",
    "csv_label = csv[\"Name\"]\n",
    "\n",
    "# 학습 전용 데이터와 테스트 전용 데이터로 나누기 --- (※3)\n",
    "train_data, test_data, train_label, test_label = \\\n",
    "train_test_split(csv_data, csv_label)\n",
    "\n",
    "# 데이터 학습시키고 예측하기 \n",
    "nb = GaussianNB()\n",
    "nb = MultinomialNB()\n",
    "\n",
    "nb.fit(train_data, train_label)\n",
    "pre = nb.predict(test_data)\n",
    "\n",
    "# 정답률 구하기 \n",
    "ac_score = metrics.accuracy_score(test_label, pre)\n",
    "print(\"정답률 =\", ac_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#assigning predictor and target variables\n",
    "x= np.array([[-3,7],[1,5], [1,2], [-2,0], [2,3], [-4,0], [-1,1], [1,1], [-2,2], [2,7], [-4,1], [-2,7]])\n",
    "y = np.array([3, 3, 3, 3, 4, 3, 3, 4, 3, 4, 4, 4])\n",
    "#Create a Gaussian Classifier\n",
    "model = GaussianNB()\n",
    "#model = MultinomialNB()\n",
    "\n",
    "# Train the model using the training sets \n",
    "model.fit(x, y)\n",
    "\n",
    "#Predict Output \n",
    "predicted= model.predict([[1,2],[3,4]])\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "각 시행의 정답률 = [0.96666667 1.         0.96666667 0.96666667 1.        ]\n",
      "평균 정답률 = 0.9800000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hjkim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "c:\\users\\hjkim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "c:\\users\\hjkim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "c:\\users\\hjkim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "c:\\users\\hjkim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm, metrics, model_selection\n",
    "import random, re\n",
    "# 붓꽃의 CSV 데이터 읽어 들이기 \n",
    "csv = pd.read_csv('iris.csv')\n",
    "\n",
    "# 리스트를 훈련 전용 데이터와 테스트 전용 데이터로 분할하기 \n",
    "data = csv[[\"SepalLength\",\"SepalWidth\",\"PetalLength\",\"PetalWidth\"]]\n",
    "label = csv[\"Name\"]\n",
    "print(len(data))\n",
    "\n",
    "clf = svm.SVC()\n",
    "scores = model_selection.cross_val_score(clf, data, label, cv=5)\n",
    "\n",
    "print(\"각 시행의 정답률 =\", scores)\n",
    "print(\"평균 정답률 =\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SepalLength  SepalWidth  PetalLength  PetalWidth         Name\n",
      "1          4.9         3.0          1.4         0.2  Iris-setosa\n",
      "2          4.7         3.2          1.3         0.2  Iris-setosa\n",
      "3          4.6         3.1          1.5         0.2  Iris-setosa\n",
      "4          5.0         3.6          1.4         0.2  Iris-setosa\n",
      "5          5.4         3.9          1.7         0.4  Iris-setosa\n",
      "6          4.6         3.4          1.4         0.3  Iris-setosa\n",
      "7          5.0         3.4          1.5         0.2  Iris-setosa\n",
      "8          4.4         2.9          1.4         0.2  Iris-setosa\n",
      "9          4.9         3.1          1.5         0.1  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm, metrics, model_selection\n",
    "import random, re\n",
    "# 붓꽃의 CSV 데이터 읽어 들이기 \n",
    "csv = pd.read_csv('iris.csv')\n",
    "print(csv[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
