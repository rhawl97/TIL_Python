{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import Flatten\n",
    "\n",
    "max_features = 20000\n",
    "text_max_words = 200\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "x_val = x_train[20000:]\n",
    "y_val = y_train[20000:]\n",
    "x_train = x_train[:20000]\n",
    "y_train = y_train[:20000]\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=text_max_words)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=text_max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=text_max_words)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=text_max_words)) #128차원의 임베딩 벡터 #문서 하나당 200개 단어 \n",
    "#output shape: 200*128 \n",
    "model.add(Flatten()) #flattten 하면 200*128 = 25600\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "hist = model.fit(x_train, y_train, epochs=2, batch_size=64, validation_data=(x_val, y_val))\n",
    "\n",
    "# 모델 평가\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=64)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 200, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               6553856   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 9,146,881\n",
      "Trainable params: 9,146,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 66s 3ms/step - loss: 0.4173 - acc: 0.7927 - val_loss: 0.3041 - val_acc: 0.8714\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 73s 4ms/step - loss: 0.0494 - acc: 0.9838 - val_loss: 0.4975 - val_acc: 0.8482\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 9s 359us/step\n",
      "## evaluation loss and_metrics ##\n",
      "[0.5086848535060883, 0.8418]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 200, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 198, 256)          98560     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,585\n",
      "Trainable params: 2,691,585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.layers import Flatten, Dropout\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "max_features = 20000\n",
    "text_max_words = 200\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "x_val = x_train[20000:]\n",
    "y_val = y_train[20000:]\n",
    "x_train = x_train[:20000]\n",
    "y_train = y_train[:20000]\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=text_max_words)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=text_max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=text_max_words)\n",
    "\n",
    "##p.64확인## ##각 숫자 시험문제##\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=text_max_words))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(256, 3, padding='valid', activation='relu', strides=1)) #conv1D! 일차원적인 convolution!\n",
    "#256: filter 개수 3: filter size -> kernel 3개 당 하나의 새로운 text data가 나옴 \n",
    "#output: 각 커널에 대해서 sequence가 생기므로 256개 depth를 가지는 text sequence data가 생김!! \n",
    "##각 문서마다 200개 단어니까 (200-3)+1=198(하나의 sequence dimension) 인게 256개 생기는 것!\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(x_train, y_train, epochs=2, batch_size=64, validation_data=(x_val, y_val))\n",
    "\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=64)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 115s 6ms/step - loss: 0.4672 - acc: 0.7567 - val_loss: 0.3048 - val_acc: 0.8702\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 0.2155 - acc: 0.9142 - val_loss: 0.2577 - val_acc: 0.9004\n",
      "25000/25000 [==============================] - 27s 1ms/step\n",
      "## evaluation loss and_metrics ##\n",
      "[0.2655917287492752, 0.8875200000190735]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  RNN 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 163s 8ms/step - loss: 0.4391 - acc: 0.7931 - val_loss: 0.3835 - val_acc: 0.8298\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 158s 8ms/step - loss: 0.2288 - acc: 0.9139 - val_loss: 0.3252 - val_acc: 0.8620\n",
      "25000/25000 [==============================] - 43s 2ms/step\n",
      "## evaluation loss and_metrics ##\n",
      "[0.3418483753299713, 0.855480000038147]\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.layers import Flatten\n",
    "\n",
    "max_features = 20000\n",
    "text_max_words = 200\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "x_val = x_train[20000:]\n",
    "y_val = y_train[20000:]\n",
    "x_train = x_train[:20000]\n",
    "y_train = y_train[:20000]\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=text_max_words)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=text_max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=text_max_words)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))   # 128 차원의 embedding vector 생성\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "hist = model.fit(x_train, y_train, epochs=2, batch_size=64, validation_data=(x_val, y_val))\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=64)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RCNN = RNN + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.layers import Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "max_features = 20000\n",
    "text_max_words = 200\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "x_val = x_train[20000:]\n",
    "y_val = y_train[20000:]\n",
    "x_train = x_train[:20000]\n",
    "y_train = y_train[:20000]\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=text_max_words)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=text_max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=text_max_words)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=text_max_words))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(256, 3, padding='valid', activation='relu', strides=1))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(x_train, y_train, epochs=2, batch_size=64, validation_data=(x_val, y_val))\n",
    "\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=64)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
